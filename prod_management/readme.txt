prod_management directory was created to create & test initial db mechanics for flask project. Contains 5 different files
-basic_rnn_model: contains same forward propogation arithmatic in a class based system as seen in jupyter notebooks where training took place
-db_refresh: contains same fetching/writing functions as found in dataflow directory accompanied by some db startup functions (db_norm_metrics)
-load_data: used to store any postgresql queries.
-screening_alg: class based system identical to dataflow directory. Purpose is to hold the rule based filtration system as an intermediate step between database refreshes and the ML being deployed. Further documentation can be found in dbg 'docs' tile.
-test_job: file was used to contain all functions required for CRON job to refresh data on a daily basis. ticker_check identifies any changes to the security list on the s&p500 index. If any new securities are found the previous 60 days worth of data are automatically added to the db for analysis and if any securities are no loger on the list refreshes for them won't continue. dupe_check and missing_data_check are two flags used to identify if data already exists in the db and should be skipped or if we're missing a few days worth of data and to save that in a missing_data table. daily_refresh uses the read/write functions in db_refresh to ensure that all data actually gets added into db on a day to day basis. buy_search uses a combination of the screening_alg class based system to identify market opportunities, if triggered employs the ml algorithm found in basic_rnn_model to generate a prediction and save to db. sell_search identifies all open trades and runs the sell critieria found in the same screening_alg class to identify selling flags & write results to db. backfill was designed to automatically run buy_search & sell_search through x period of time to backfill db with ml workflow results. load_database was designed to backfill entire db with data so we could run backfill function. backfill_trade_data was developed after inconsistencies in the datasource were identified. The workflow first identified all tickers with missing data. With those tickers I pull from our datasource. I then check for duplicate data & cross reference the last data received with the current data available & write all missing data to db. In most cases the db is only missing one day worth of data but in some instances multiple days are missing hence the requirement to compare last data received and write all data available after last data received. The missing data table is then refreshed, so when the job runs the next hour in an effort to scoop up any data that wasn't available at 4:30 I don't waste a ton of time querying tickers that I already know are up to date. The last step is to run the buy_search & sell_search functions & update the db to identify the last time the job was sucessfully run. final function is full_test_job. consider it a zzz function, does not handle missing data well hence the requirement for backfill_trade_data.