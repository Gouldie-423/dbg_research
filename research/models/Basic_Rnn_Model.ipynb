{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d8f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import load\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab896f29",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbd21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_data = '/Users/timothygould/dbg_research/research/training_data/vanilla_data/'\n",
    "standard_norm = '/Users/timothygould/dbg_research/research/training_data/standard_norm/'\n",
    "zero_to_one = '/Users/timothygould/dbg_research/research/training_data/zero_to_one_norm/'\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    train_x=np.load(f'{path}train_x.npy',allow_pickle=True,fix_imports=True,encoding='latin1')\n",
    "    train_y=np.load(f'{path}train_y.npy',allow_pickle=True,fix_imports=True,encoding='latin1')\n",
    "    test_x=np.load(f'{path}test_x.npy',allow_pickle=True,fix_imports=True,encoding='latin1')\n",
    "    test_y=np.load(f'{path}test_y.npy',allow_pickle=True,fix_imports=True,encoding='latin1')\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db24ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y = load_data(zero_to_one)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f5f8c",
   "metadata": {},
   "source": [
    "# Model Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb1d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    #each algorithm is instantiated with all hyperparamaters required for adam optimization & l2 regularization.\n",
    "    #betas hyperparameter is passed in as a list [beta1,beta2]\n",
    "    #layers_dims is a dictionary containing dimensions for algorithm matrix dimensions\n",
    "    def __init__(self,learning_rate,betas,lambd,train_data,train_labels,test_data,test_labels,num_epochs,layers_dims,attempt):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_x = train_data\n",
    "        self.train_y = train_labels\n",
    "        self.test_x = test_data\n",
    "        self.test_y = test_labels\n",
    "        self.num_epochs = num_epochs\n",
    "        self.layers_dims = layers_dims\n",
    "        \n",
    "    #can either be used to randomly initialize algorithm parameters or use standard method supplied by pytorch (if alt_init = True)\n",
    "    #rnn params are missing Wya and By params b/c the rnn goes right into a feed forward network and therefore said params weren't used. Code is commented out below\n",
    "    def initialize_params(self,alt_init = False):\n",
    "        nx = self.train_x.shape[0]\n",
    "        m = self.train_x.shape[1]\n",
    "        tx = self.train_x.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        \n",
    "        # RNN Params\n",
    "        for i in range(0,len(layers_dims['rnn'])):\n",
    "            self.params['Waa'+str(i+1)] = torch.rand(layers_dims['rnn'][i],layers_dims['rnn'][i],dtype = torch.float64, requires_grad = True)\n",
    "            self.params['Wax'+str(i+1)] = torch.rand(layers_dims['rnn'][i],nx,dtype = torch.float64, requires_grad = True)\n",
    "#             self.params['Wya'+str(i+1)] = torch.rand(2,layers_dims['rnn'][i],dtype = torch.float64, requires_grad = True)\n",
    "            self.params['ba'+str(i+1)] = torch.rand(layers_dims['rnn'][i],1,dtype = torch.float64, requires_grad = True)\n",
    "#             self.params['by'+str(i+1)] = torch.rand(layers_dims['rnn'][i],1,dtype = torch.float64, requires_grad = True)\n",
    "        \n",
    "        # FF Params\n",
    "        for i in range(1,len(layers_dims['ff'])):\n",
    "            self.params['W'+str(i)] = torch.rand(layers_dims['ff'][i],layers_dims['ff'][i-1],dtype = torch.float64,requires_grad = True)\n",
    "            self.params['b'+str(i)] = torch.rand(layers_dims['ff'][i],1,dtype = torch.float64,requires_grad = True)\n",
    "      \n",
    "        if alt_init:\n",
    "            for i in self.params.keys():\n",
    "                self.params[i] = nn.init.xavier_uniform_(self.params[i])\n",
    "                \n",
    "        return self.params\n",
    "   \n",
    "    #takes output from rnn layer & calculates the predicted y value by calculating activations on weighted sums through each layer\n",
    "    def ff_forward(self,a):\n",
    "        a_next = a #need this here so a_next can be overridden inbetween layers\n",
    "        for i in range(1,len(layers_dims['ff'])-1):\n",
    "            w = self.params['W'+str(i)]\n",
    "            b = self.params['b'+str(i)]\n",
    "            z = torch.matmul(w,a_next)+b\n",
    "            a_next = torch.tanh(z)\n",
    "        \n",
    "        w = self.params['W'+str(len(layers_dims['ff'])-1)]\n",
    "        b = self.params['b'+str(len(layers_dims['ff'])-1)]\n",
    "        z = torch.matmul(w,a_next)+b\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    #calculates activation on weighted sum for RNN configuration. Does not loop through each layer individually within itself.\n",
    "    #layer needs to be specified within argument. Currently using 2 layer RNN\n",
    "    def rnn_cell_forward(self,layer,a_prev,xt):\n",
    "        \n",
    "        Wax = self.params['Wax'+str(layer)]\n",
    "        Waa = self.params['Waa'+str(layer)]\n",
    "        ba = self.params['ba'+str(layer)]\n",
    "\n",
    "        z = torch.matmul(Waa,a_prev)+torch.matmul(Wax,xt)+ba\n",
    "        a_next = torch.tanh(z)\n",
    "        \n",
    "        return a_next\n",
    "    \n",
    "    #arithmetic for rnn deep learning forward propogation\n",
    "    def forward_pass(self):\n",
    "        \n",
    "        tx = self.train_x.shape[2]\n",
    "        \n",
    "        a_next1 = torch.zeros(self.train_x.shape[0],self.train_x.shape[1],dtype = torch.float64)\n",
    "        a_next2 = torch.zeros(self.train_x.shape[0],self.train_x.shape[1],dtype = torch.float64)\n",
    "        \n",
    "\n",
    "        for xt in range(0,tx):\n",
    "            a_next1 =  self.rnn_cell_forward(1,a_next1,self.train_x[:,:,xt])\n",
    "            \n",
    "            a_next2 = self.rnn_cell_forward(2,a_next2,a_next1)\n",
    "            \n",
    "        y_pred = self.ff_forward(a_next2)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self):\n",
    "        tx = self.train_x.shape[2]\n",
    "        \n",
    "        a_next1 = torch.zeros(self.test_x.shape[0],self.test_x.shape[1],dtype = torch.float64)\n",
    "        a_next2 = torch.zeros(self.test_x.shape[0],self.test_x.shape[1],dtype = torch.float64)\n",
    "        \n",
    "        \n",
    "        for xt in range(0,tx):\n",
    "\n",
    "            a_next1 =  self.rnn_cell_forward(1,a_next1,self.test_x[:,:,xt])\n",
    "            \n",
    "            a_next2 = self.rnn_cell_forward(2,a_next2,a_next1)\n",
    "            \n",
    "        y_pred = self.ff_forward(a_next2)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    #quicly calulates sum of gradients. Used in tensorboard to help identify if gradients may be dissapearing on exploding when tuning.\n",
    "    def grad_sum(self):\n",
    "        \n",
    "        grad_sum = 0\n",
    "        \n",
    "        for matrix in self.params.keys():\n",
    "            grad_sum += sum(sum(self.params[matrix].grad))\n",
    "            \n",
    "        return grad_sum\n",
    "    \n",
    "    #attempt at using tensorboard histogram to observe false positive,true positive,false negative, and true negative \n",
    "    #rates at each threshold from .1-.9.\n",
    "    #used to \n",
    "    def tb_metrics(self,y_pred,test=False):\n",
    "        \n",
    "        baseline = 0\n",
    "        self.model_output = {}\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "\n",
    "        if not test:\n",
    "            labels = self.train_y.detach().numpy()\n",
    "        if test:\n",
    "            labels = self.test_y.detach().numpy()\n",
    "                \n",
    "        total_false_pos = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        trades_at_gradient = np.round(y_pred,1)\n",
    "        false_pos_array = np.zeros((0,0))\n",
    "        \n",
    "        while round(baseline,2) < .9:\n",
    "            #rounding output to .1 for each gradient between .1 and .9 to check accuracy at each gradient\n",
    "            baseline+=.1\n",
    "            self.model_output[str(round(baseline,2))] = np.where(np.round(y_pred,1) == round(baseline,2),1,0)\n",
    "            \n",
    "        for i in self.model_output.keys():\n",
    "            baseline = float(i)\n",
    "            num_samples = np.sum(np.where(np.round(y_pred,1) == baseline,1,0),axis=1)\n",
    "            test = np.where((self.model_output[i]==1) & (labels == 0),baseline,0)\n",
    "            false_positive = np.sum(np.where((self.model_output[i] == 1) & (labels==0),1,0),axis=1)\n",
    "            true_positive = np.sum(np.where((self.model_output[i]==1) & (labels==1),1,0),axis=1)\n",
    "            true_negative = np.sum(np.where((self.model_output[i]==0) & (labels==0),1,0),axis=1)\n",
    "            false_negative = np.sum(np.where((self.model_output[i]==0) & (labels==1),1,0),axis=1)\n",
    "            \n",
    "            total_false_pos += false_positive\n",
    "            total_samples += num_samples\n",
    "            false_pos_array = np.concatenate((false_pos_array,test), axis=None)\n",
    "        \n",
    "        false_pos_array2 = false_pos_array[false_pos_array != 0.]\n",
    "        false_positive = total_false_pos/total_samples\n",
    "        \n",
    "        return false_positive,trades_at_gradient,false_pos_array2\n",
    "    \n",
    "    #used to save and export weights. Path may need to be modified depending on what new batch of tests took place.\n",
    "    def weights_export(self,alg_name):\n",
    "        for k,v in self.params.items():\n",
    "            torch.save(v,f'/Users/timothygould/dbg_research/research/models/15_day_lookback/{alg_name}/{k}.pt')\n",
    "    \n",
    "    #used to identify the win rate/ loss rate for each threshold (.1-.9 range) along with the percentage of total trades \n",
    "    #within each threshold\n",
    "    #zzz- was not used in final model\n",
    "    def final_metrics(self,trades_at_gradient,test = False):\n",
    "        if not test:\n",
    "            labels = self.train_y.detach().numpy()\n",
    "            num_samples = self.train_x.shape[1]\n",
    "        if test:\n",
    "            labels = self.test_y.detach().numpy()\n",
    "            num_samples = self.test_x.shape[1]\n",
    "            \n",
    "        plt_labels = ['.1','.2','.3','.4','.5','.6','.7','.8','.9']\n",
    "        total_samples = []\n",
    "        neg_samples = []\n",
    "        pos_samples = []\n",
    "        #generating quantity of trades at each output gradient\n",
    "        for output in plt_labels:\n",
    "            print(output)\n",
    "            total_samples.append(np.sum(np.where(trades_at_gradient==float(output),1,0)))\n",
    "            neg_samples.append(np.sum(np.where((trades_at_gradient==float(output)) & (labels==0),1,0)))\n",
    "            pos_samples.append(np.sum(np.where((trades_at_gradient==float(output)) & (labels==1),1,0)))\n",
    "\n",
    "        win_rate = [(pos_samples[i]/total_samples[i])*100 if total_samples[i]>0 else 0 for i in range(0,len(plt_labels))]\n",
    "        lose_rate = [(neg_samples[i]/total_samples[i])*100 if total_samples[i]>0 else 0 for i in range(0,len(plt_labels))]\n",
    "        pct_of_trades = [(quantity_of_trades[i]/num_samples)*100 if total_samples[i]>0 else 0 for i in range(0,len(plt_labels))]\n",
    "        \n",
    "        return win_rate,lose_rate,pct_of_trades\n",
    "    \n",
    "    #much like final_metrics was not used in final model\n",
    "    #zzz\n",
    "    def final_metrics2(self,y_pred,test=False):\n",
    "\n",
    "        plt_labels = [0,.5,.75,1]\n",
    "        \n",
    "        if not test:\n",
    "            labels = self.train_y.detach().numpy()\n",
    "            num_samples = self.train_x.shape[1]\n",
    "        if test:\n",
    "            labels = self.test_y.detach().numpy()\n",
    "            num_samples = self.test_x.shape[1]\n",
    "\n",
    "        total_samples = []\n",
    "        pos_samples = []\n",
    "        neg_samples = []\n",
    "        for i in range(0,len(plt_labels)-1):\n",
    "            quantity_of_trades = np.sum(np.where((y_pred>plt_labels[i])&(y_pred<plt_labels[i+1]),1,0))\n",
    "            win_rate = np.sum(np.where((y_pred>plt_labels[i])&(y_pred<plt_labels[i+1])&(labels==1),1,0))\n",
    "            lose_rate = np.sum(np.where((y_pred>plt_labels[i])&(y_pred<plt_labels[i+1])&(labels==0),1,0))\n",
    "            total_samples.append(quantity_of_trades)\n",
    "            pos_samples.append(win_rate)\n",
    "            neg_samples.append(lose_rate)\n",
    "\n",
    "        win_rate = [(pos_samples[i]/total_samples[i])*100 if total_samples[i]>0 else 0 for i in range(0,len(pos_samples))]\n",
    "        lose_rate = [(neg_samples[i]/total_samples[i])*100 if total_samples[i]>0 else 0 for i in range(0,len(neg_samples))]\n",
    "        pct_of_trades = [(total_samples[i]/num_samples)*100 if total_samples[i]>0 else 0 for i in range(0,len(total_samples))]\n",
    "\n",
    "        return win_rate,lose_rate,pct_of_trades\n",
    "    \n",
    "    #zzz-attempt at using matplotlib to identify different decision barriers for threshold optimization. \n",
    "    #not super helpful, will attempt such a method again.\n",
    "    def graph(self,trades_at_gradient,test_trades_at_gradient,attempt):\n",
    "        plt_labels = ['.1','.2','.3','.4','.5','.6','.7','.8','.9']\n",
    "        plt_labels2 = ['0-.5','.5-.75','.75-1']\n",
    "        width = .35\n",
    "        ind = np.arange(3)\n",
    "        \n",
    "        win_rate_train,lose_rate_train,pct_of_trades_train = self.final_metrics2(trades_at_gradient,test=False)\n",
    "        win_rate_test,lose_rate_test,pct_of_trades_test = self.final_metrics2(test_trades_at_gradient,test=True)\n",
    "        \n",
    "        win_rate_avg = [(win_rate_train[i]-win_rate_test[i]) for i in range(0,len(plt_labels2))]\n",
    "        \n",
    "        plt.title(f'Percent of Trades by Output Gradient V{attempt}')\n",
    "        plt.ylabel('Percent of Trades')\n",
    "        plt.xlabel('Output Gradient')\n",
    "        plt.ylim(0,60)\n",
    "        plt.yticks(np.arange(0,60,5))\n",
    "        plt.bar(plt_labels2, pct_of_trades_train, width, label='Train')\n",
    "        plt.bar(ind + width, pct_of_trades_test, width, label='Test')\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "#         plt.savefig(f'/Users/timothygould/dbg_research/research/models/new_threshold_test/V{attempt}/Trades_by_Output')\n",
    "\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.title(f'Win Rate by Output Gradient')\n",
    "        plt.ylabel('Wins & Losses as a Percent')\n",
    "        plt.xlabel('Output Gradient')\n",
    "        plt.ylim(0,100)\n",
    "        plt.yticks(np.arange(0,100,10))\n",
    "        plt.bar(plt_labels2, win_rate_train, width, label='Train Wins', color='blue')\n",
    "        plt.bar(plt_labels2,lose_rate_train,width,label = 'Train Losses',color='red',bottom = win_rate_train)\n",
    "        plt.bar(ind+width, win_rate_test, width, label='Test Wins', color='purple',)\n",
    "        plt.bar(ind+width, lose_rate_test, width, label='Test Losses', color='orange',bottom = win_rate_test)\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "#         plt.savefig(f'/Users/timothygould/dbg_research/research/models/new_threshold_test/V{attempt}/Win_Rates')\n",
    "        \n",
    "        plt.clf()\n",
    "        \n",
    "        win_rate_avg = sum(win_rate_avg)/len(plt_labels2)\n",
    "        return win_rate_avg\n",
    "        \n",
    "        #runs algorithm & writes observation metrics to tensorboard for further review. \n",
    "    def run(self):\n",
    "        #previously saved naming conventions for different paramater tests\n",
    "        #tensorboard writer lr_{learning_rate}_xavier_adam_{beta}\n",
    "        #tensorboard/final_models/runs/v_{attempt}\n",
    "        \n",
    "        writer = SummaryWriter(f'tensorboard/15_day_lookback/runs/v_{attempt}_lr_{learning_rate}_{beta}_{lambd}')\n",
    "        \n",
    "        self.params = self.initialize_params(alt_init = True)\n",
    "        loss = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.params.values(),lr=learning_rate,betas=beta,weight_decay=lambd)\n",
    "        \n",
    "        for epoch in range(0,num_epochs):\n",
    "            y_pred = self.forward_pass()\n",
    "            output = loss(y_pred,self.train_y)\n",
    "            writer.add_scalar(\"Loss/train\", output, epoch)\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            #every ten epochs (iterations) observation metrics are added to tensorboard\n",
    "            if epoch % 10 == 0:\n",
    "                false_positive,trades_at_gradient,false_pos_array = self.tb_metrics(y_pred)\n",
    "                grad_sum = self.grad_sum()\n",
    "                \n",
    "                #writing actual observation metrics\n",
    "                writer.add_scalar('grad sum',grad_sum,epoch)\n",
    "                writer.add_scalar('false positive',false_positive,epoch)\n",
    "                writer.add_histogram('trades at gradient',trades_at_gradient,epoch)\n",
    "                writer.add_histogram('false_pos_rate',false_pos_array,epoch)\n",
    "                for k,v in self.params.items():\n",
    "                    writer.add_histogram(k,v,epoch)\n",
    "                \n",
    "            #used later on to manually lower the learning rate to improve convergence when the output (cost) went below .45\n",
    "            if output< .45:\n",
    "                self.learning_rate = self.learning_rate*.75\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        test_pred = self.predict()\n",
    "        test_false_positive,test_trades_at_gradient,test_false_pos_array = self.tb_metrics(test_pred,test = True)\n",
    "        final_output = loss(test_pred,self.test_y)\n",
    "            \n",
    "        win_rate_avg = self.graph(y_pred,test_pred,attempt)\n",
    "        writer.add_scalar('average_train_test_diff_by_gradient',win_rate_avg,0)\n",
    "        self.weights_export(f'V{attempt}')\n",
    "\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15333863",
   "metadata": {},
   "source": [
    "# Random Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_gen(num_samples,custom_range = None):\n",
    "    learning_rates = []\n",
    "    base = [10,100]\n",
    "    \n",
    "    if custom_range != None:\n",
    "        for i in range(0,num_samples):\n",
    "            test = random.uniform(custom_range[0],custom_range[1])\n",
    "            learning_rates.append(test)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        for base in base:\n",
    "            for i in range(0,num_samples):\n",
    "                test = -5 *random.uniform(0,1)\n",
    "                learning_rate = base**test\n",
    "                learning_rates.append(learning_rate)\n",
    "    return learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834018ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_gen(num_samples,beta_range=None):\n",
    "    betas = []\n",
    "    \n",
    "    if beta_range != None:\n",
    "        for i in range(0,num_samples):\n",
    "            beta1 = random.uniform(beta_range[0],beta_range[1])\n",
    "            beta2 = random.uniform(beta_range[2],beta_range[3])\n",
    "            beta = [beta1,beta2]\n",
    "            betas.append(beta)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for i in range(0,num_samples):\n",
    "            beta1 = random.uniform(0,.1)\n",
    "            beta1 = 1-beta1\n",
    "            beta2 = random.uniform(0,.01)\n",
    "            beta2 = 1-beta2\n",
    "            beta = [beta1,beta2]\n",
    "            betas.append(beta)\n",
    "    return betas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f65069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after enough tests were run, certain high and low params could be identified for beta1 and beta2 (such an example is seen below)\n",
    "def adam_params(num_samples):\n",
    "\n",
    "    betas = beta_gen(num_samples,[.956209,.9281061,.996163,.99277001])\n",
    "    learning_rates = learning_rate_gen(num_samples,[.002672,.0059903])\n",
    "\n",
    "    \n",
    "    return learning_rates,betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa5579",
   "metadata": {},
   "source": [
    "# Generating Random Splits of Train/Test Data to Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a8de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally train/test data was split on predetermined boundaries. Data was combined and shuffled before creating new splits\n",
    "def train_test_split(train_x,test_x,train_y,test_y):\n",
    "    all_data = torch.cat((train_x,test_x),1)\n",
    "    all_labels = torch.cat((train_y,test_y),1)\n",
    "    \n",
    "    np.random.seed(3)\n",
    "\n",
    "    t = np.random.permutation(all_data.shape[1])\n",
    "\n",
    "    new_data = all_data[:,t,:]\n",
    "    new_labels = all_labels[:,t]\n",
    "    \n",
    "    new_train_x = all_data[:,395:,:]\n",
    "    \n",
    "    new_train_y = all_labels[:,395:]\n",
    "    \n",
    "    new_test_x = all_data[:,:395,:]\n",
    "    \n",
    "    new_test_y = all_labels[:,:395]\n",
    "    \n",
    "    return new_train_x,new_train_y,new_test_x,new_test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf03e8",
   "metadata": {},
   "source": [
    "# Random Sampling Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced91141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "newtrain_x,newtrain_y,newtest_x,newtest_y = train_test_split(train_x,test_x,train_y,test_y)\n",
    "lambds = learning_rate_gen(51,[.0001,.001])\n",
    "input_size = train_x.shape[0]\n",
    "sequence_length = train_x.shape[2] \n",
    "hidden_size = train_x.shape[0]\n",
    "num_classes = 2\n",
    "num_epochs = 5001\n",
    "num_layers = 2\n",
    "batch_size = train_x.shape[1]\n",
    "learning_rate = .0059903\n",
    "beta = [0.928106,0.995171]\n",
    "layers_dims = {'rnn':[hidden_size,hidden_size],'ff':[hidden_size,3,1]}\n",
    "\n",
    "for attempt in range(0,51):\n",
    "    lambd = lambds[attempt]\n",
    "    rnn = model(learning_rate,beta,lambd,newtrain_x,newtrain_y,newtest_x,newtest_y,num_epochs,layers_dims,attempt)\n",
    "    rnn.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0ee8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
