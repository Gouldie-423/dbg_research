four directories fall within research

data_preprocessing: directory contains four files whose primary function was to standarize, normalize, and manipulate the data into correctly formatted matricies to be used later on in the research pipeline. 
-preprocessing_config: config file for the whole directory. contains engine strings for sqlalchemy, paths for saving different training matricies. grab_data is used to query the db to pull in all realevent data ranging from the raw data itself, training labels, unique keys that represent each trade, and the normalizing metrics. ma_pct_diff is uses to standardize the moving average values (reasoning can be found in paper) prior to normalization.
-data_preprocessing: primary workspace where database data gets manipulated and transformed into pytorch tensors to be trained on. training_data is how the data is put into the matrix. First we pull all of the training data for all samples in a single dataframe. We then loop through each unique key and filter the dataframe so that only the required key remains. All of the features are placed on the 0th axis, each unique key is on the 1st axis and each time step is then looped through added on along the 2nd axis. normalize_data is a function that can normalize the data a variety of different ways (zero-to-one or standard) before returning the now manipulated dataframe so it can be used to create matricies later on. process_data is the primary function where all of the normalization & placing into matricies takes place. strings specifying what kind of nnormalization or lookback range is required. This function is used multiple times at the bottom of the script before matricies are all saved.
-normalization_vis: first two functions actually run through normalization math. norm_export_db takes in the training data & saves all of the normalizing metrics required to db so they don't need to be generated from the training set every time. Additionally when using ML its imperative that the same normaliziang metrics from the training set are used for all other datasets and future samples. Last function is export_vis. this was used to visualize the different normalization types. The differences between standard and zero-to-one normalization are expanded on within paper.
-data_preprocessing_test: file intended to cross reference both the data in the matricies and the db to ensure that everything is lined up properly. spot_check was designed to check random keys and time steps. the idea was that I could focus on larger issues that pop up more frequently looking at smaller sample sizes of data before running full_test which procedurally check each individual record. 
indicator_vis: directory contains graphs showcasing different preprocessing and normalization techniques for each of the trades identified by the rule based filtration system
models: contains both jupyter notebooks and actual weights+biases for each of the previously trained models and the accompanying tensorboard documentation. Each of the juypyter notebooks has seperate documentation within each notebook.